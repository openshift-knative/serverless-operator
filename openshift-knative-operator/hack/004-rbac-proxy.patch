diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/0-rbac-proxy.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/0-rbac-proxy.yaml
new file mode 100644
index 00000000..37558cd9
--- /dev/null
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/0-rbac-proxy.yaml
@@ -0,0 +1,314 @@
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: rbac-proxy-metrics-prom
+rules:
+  - nonResourceURLs: ["/metrics"]
+    verbs: ["get"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: rbac-proxy-metrics-prom-rb
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: rbac-proxy-metrics-prom
+subjects:
+  - kind: ServiceAccount
+    name: prometheus-k8s
+    namespace: openshift-monitoring
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: rbac-proxy-reviews-prom
+rules:
+  - apiGroups: ["authentication.k8s.io"]
+    resources:
+      - tokenreviews
+    verbs: ["create"]
+  - apiGroups: ["authorization.k8s.io"]
+    resources:
+      - subjectaccessreviews
+    verbs: ["create"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: rbac-proxy-reviews-prom-rb
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: rbac-proxy-reviews-prom
+subjects:
+  - kind: ServiceAccount
+    name: controller
+    namespace: knative-serving
+---
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.beta.openshift.io/serving-cert-secret-name: activator-sm-service-tls
+  labels:
+    name: activator-sm-service
+  name: activator-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: activator
+---
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.beta.openshift.io/serving-cert-secret-name: autoscaler-sm-service-tls
+  labels:
+    name: autoscaler-sm-service
+  name: autoscaler-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: autoscaler
+---
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.beta.openshift.io/serving-cert-secret-name: autoscaler-hpa-sm-service-tls
+  labels:
+    name: autoscaler-hpa-sm-service
+  name: autoscaler-hpa-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: autoscaler-hpa
+---
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.beta.openshift.io/serving-cert-secret-name: controller-sm-service-tls
+  labels:
+    name: controller-sm-service
+  name: controller-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: controller
+---
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.beta.openshift.io/serving-cert-secret-name: domain-mapping-sm-service-tls
+  labels:
+    name: domain-mapping-sm-service
+  name: domain-mapping-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: domain-mapping
+---
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.beta.openshift.io/serving-cert-secret-name: domainmapping-webhook-sm-service-tls
+  labels:
+    name: domainmapping-webhook-sm-service
+  name: domainmapping-webhook-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: domainmapping-webhook
+---
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.beta.openshift.io/serving-cert-secret-name: webhook-sm-service-tls
+  labels:
+    name: webhook-sm-service
+  name: webhook-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: webhook
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: activator-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+    bearerTokenSecret:
+      key: ""
+    port: https
+    scheme: https
+    tlsConfig:
+      caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
+      serverName: activator-sm-service.knative-serving.svc
+  namespaceSelector:
+    matchNames:
+    - knative-serving
+  selector:
+    matchLabels:
+      name: activator-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: autoscaler-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
+        serverName: autoscaler-sm-service.knative-serving.svc
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: autoscaler-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: autoscaler-hpa-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
+        serverName: autoscaler-hpa-sm-service.knative-serving.svc
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: autoscaler-hpa-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: controller-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
+        serverName: controller-sm-service.knative-serving.svc
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: controller-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: domain-mapping-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
+        serverName: domain-mapping-sm-service.knative-serving.svc
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: domain-mapping-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: domainmapping-webhook-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
+        serverName: domainmapping-webhook-sm-service.knative-serving.svc
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: domainmapping-webhook-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: webhook-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
+        serverName: webhook-sm-service.knative-serving.svc
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: webhook-sm-service
diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/2-serving-core.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/2-serving-core.yaml
index 8e15a4d8..81dbd549 100644
--- a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/2-serving-core.yaml
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/2-serving-core.yaml
@@ -2216,6 +2216,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/internal/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2247,6 +2249,26 @@ spec:
                   value: "activator"
             failureThreshold: 12
             initialDelaySeconds: 15
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          volumeMounts:
+            - mountPath: /etc/tls/private
+              name: secret-activator-sm-service-tls
+          resources:
+            requests:
+              memory: 20Mi
+              cpu: 10m
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--tls-cert-file=/etc/tls/private/tls.crt"
+            - "--tls-private-key-file=/etc/tls/private/tls.key"
+            - "--logtostderr=true"
+            - "--v=10"
+      volumes:
+        - name: secret-activator-sm-service-tls
+          secret:
+            secretName: activator-sm-service-tls
       # The activator (often) sits on the dataplane, and may proxy long (e.g.
       # streaming, websockets) requests.  We give a long grace period for the
       # activator to "lame duck" and drain outstanding requests before we
@@ -2361,6 +2383,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2388,6 +2412,26 @@ spec:
                 - name: k-kubelet-probe
                   value: "autoscaler"
             failureThreshold: 6
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          volumeMounts:
+            - mountPath: /etc/tls/private
+              name: secret-autoscaler-sm-service-tls
+          resources:
+            requests:
+              memory: 20Mi
+              cpu: 10m
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--tls-cert-file=/etc/tls/private/tls.crt"
+            - "--tls-private-key-file=/etc/tls/private/tls.key"
+            - "--logtostderr=true"
+            - "--v=10"
+      volumes:
+        - name: secret-autoscaler-sm-service-tls
+          secret:
+            secretName: autoscaler-sm-service-tls
 ---
 apiVersion: v1
 kind: Service
@@ -2485,6 +2529,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/internal/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2497,6 +2543,26 @@ spec:
               containerPort: 9090
             - name: profiling
               containerPort: 8008
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          volumeMounts:
+            - mountPath: /etc/tls/private
+              name: secret-controller-sm-service-tls
+          resources:
+            requests:
+              memory: 20Mi
+              cpu: 10m
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--tls-cert-file=/etc/tls/private/tls.crt"
+            - "--tls-private-key-file=/etc/tls/private/tls.key"
+            - "--logtostderr=true"
+            - "--v=10"
+      volumes:
+        - name: secret-controller-sm-service-tls
+          secret:
+            secretName: controller-sm-service-tls
 ---
 apiVersion: v1
 kind: Service
@@ -2647,6 +2713,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/internal/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2673,6 +2741,26 @@ spec:
             !!merge <<: *probe
             failureThreshold: 6
             initialDelaySeconds: 20
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          volumeMounts:
+            - mountPath: /etc/tls/private
+              name: secret-webhook-sm-service-tls
+          resources:
+            requests:
+              memory: 20Mi
+              cpu: 10m
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--tls-cert-file=/etc/tls/private/tls.crt"
+            - "--tls-private-key-file=/etc/tls/private/tls.key"
+            - "--logtostderr=true"
+            - "--v=10"
+      volumes:
+        - name: secret-webhook-sm-service-tls
+          secret:
+            secretName: webhook-sm-service-tls
       # Our webhook should gracefully terminate by lame ducking first, set this to a sufficiently
       # high value that we respect whatever value it has configured for the lame duck grace period.
       terminationGracePeriodSeconds: 300
diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/3-serving-hpa.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/3-serving-hpa.yaml
index 5c2ae1d5..4cc33302 100644
--- a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/3-serving-hpa.yaml
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/3-serving-hpa.yaml
@@ -67,6 +67,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -79,6 +81,26 @@ spec:
               containerPort: 9090
             - name: profiling
               containerPort: 8008
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          volumeMounts:
+            - mountPath: /etc/tls/private
+              name: secret-autoscaler-hpa-sm-service-tls
+          resources:
+            requests:
+              memory: 20Mi
+              cpu: 10m
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--tls-cert-file=/etc/tls/private/tls.crt"
+            - "--tls-private-key-file=/etc/tls/private/tls.key"
+            - "--logtostderr=true"
+            - "--v=10"
+      volumes:
+        - name: secret-autoscaler-hpa-sm-service-tls
+          secret:
+            secretName: autoscaler-hpa-sm-service-tls
 ---
 apiVersion: v1
 kind: Service
diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/5-serving-domainmapping.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/5-serving-domainmapping.yaml
index a980952f..bcee5d5d 100644
--- a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/5-serving-domainmapping.yaml
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.20.0/5-serving-domainmapping.yaml
@@ -175,6 +175,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -187,6 +189,26 @@ spec:
               containerPort: 9090
             - name: profiling
               containerPort: 8008
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          volumeMounts:
+            - mountPath: /etc/tls/private
+              name: secret-domain-mapping-sm-service-tls
+          resources:
+            requests:
+              memory: 20Mi
+              cpu: 10m
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--tls-cert-file=/etc/tls/private/tls.crt"
+            - "--tls-private-key-file=/etc/tls/private/tls.key"
+            - "--logtostderr=true"
+            - "--v=10"
+      volumes:
+        - name: secret-domain-mapping-sm-service-tls
+          secret:
+            secretName: domain-mapping-sm-service-tls
 
 ---
 # Copyright 2020 The Knative Authors
@@ -265,6 +287,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
           ports:
@@ -286,6 +310,26 @@ spec:
             !!merge <<: *probe
             failureThreshold: 6
             initialDelaySeconds: 20
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          volumeMounts:
+            - mountPath: /etc/tls/private
+              name: secret-domainmapping-webhook-sm-service-tls
+          resources:
+            requests:
+              memory: 20Mi
+              cpu: 10m
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--tls-cert-file=/etc/tls/private/tls.crt"
+            - "--tls-private-key-file=/etc/tls/private/tls.key"
+            - "--logtostderr=true"
+            - "--v=10"
+      volumes:
+        - name: secret-domainmapping-webhook-sm-service-tls
+          secret:
+            secretName: domainmapping-webhook-sm-service-tls
       # Our webhook should gracefully terminate by lame ducking first, set this to a sufficiently
       # high value that we respect whatever value it has configured for the lame duck grace period.
       terminationGracePeriodSeconds: 300
