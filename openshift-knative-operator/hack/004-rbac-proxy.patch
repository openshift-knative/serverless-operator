diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/2-serving-core.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/2-serving-core.yaml
index 3c213343..9724c974 100644
--- a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/2-serving-core.yaml
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/2-serving-core.yaml
@@ -2226,6 +2226,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/internal/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2257,6 +2259,13 @@ spec:
                   value: "activator"
             failureThreshold: 12
             initialDelaySeconds: 15
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--logtostderr=true"
+            - "--v=10"
       # The activator (often) sits on the dataplane, and may proxy long (e.g.
       # streaming, websockets) requests.  We give a long grace period for the
       # activator to "lame duck" and drain outstanding requests before we
@@ -2371,6 +2380,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2398,6 +2409,13 @@ spec:
                 - name: k-kubelet-probe
                   value: "autoscaler"
             failureThreshold: 6
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--logtostderr=true"
+            - "--v=10"
 ---
 apiVersion: v1
 kind: Service
@@ -2495,6 +2513,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/internal/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2507,6 +2527,13 @@ spec:
               containerPort: 9090
             - name: profiling
               containerPort: 8008
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--logtostderr=true"
+            - "--v=10"
 ---
 apiVersion: v1
 kind: Service
@@ -2655,6 +2682,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -2681,6 +2710,13 @@ spec:
             !!merge <<: *probe
             failureThreshold: 6
             initialDelaySeconds: 20
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--logtostderr=true"
+            - "--v=10"
       # Our webhook should gracefully terminate by lame ducking first, set this to a sufficiently
       # high value that we respect whatever value it has configured for the lame duck grace period.
       terminationGracePeriodSeconds: 300
diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/3-serving-hpa.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/3-serving-hpa.yaml
index 83b88232..5a0a0e15 100644
--- a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/3-serving-hpa.yaml
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/3-serving-hpa.yaml
@@ -67,6 +67,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
             readOnlyRootFilesystem: true
@@ -79,6 +81,13 @@ spec:
               containerPort: 9090
             - name: profiling
               containerPort: 8008
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--logtostderr=true"
+            - "--v=10"
 ---
 apiVersion: v1
 kind: Service
diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/5-serving-domainmapping.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/5-serving-domainmapping.yaml
index d0b8b2bb..508c6211 100644
--- a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/5-serving-domainmapping.yaml
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/5-serving-domainmapping.yaml
@@ -187,6 +187,13 @@ spec:
               containerPort: 9090
             - name: profiling
               containerPort: 8008
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--logtostderr=true"
+            - "--v=10"

 ---
 # Copyright 2020 The Knative Authors
@@ -265,6 +272,8 @@ spec:
             # TODO(https://github.com/knative/pkg/pull/953): Remove stackdriver specific config
             - name: METRICS_DOMAIN
               value: knative.dev/serving
+            - name: METRICS_PROMETHEUS_HOST
+              value: "127.0.0.1"
           securityContext:
             allowPrivilegeEscalation: false
           ports:
@@ -286,6 +295,13 @@ spec:
             !!merge <<: *probe
             failureThreshold: 6
             initialDelaySeconds: 20
+        - name: kube-rbac-proxy
+          image: quay.io/brancz/kube-rbac-proxy:v0.8.0
+          args:
+            - "--secure-listen-address=0.0.0.0:8444"
+            - "--upstream=http://127.0.0.1:9090/"
+            - "--logtostderr=true"
+            - "--v=10"
       # Our webhook should gracefully terminate by lame ducking first, set this to a sufficiently
       # high value that we respect whatever value it has configured for the lame duck grace period.
       terminationGracePeriodSeconds: 300
diff --git a/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/7-rbac-proxy.yaml b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/7-rbac-proxy.yaml
new file mode 100644
index 00000000..11005a1e
--- /dev/null
+++ b/openshift-knative-operator/cmd/operator/kodata/knative-serving/0.19.0/7-rbac-proxy.yaml
@@ -0,0 +1,293 @@
+---
+apiVersion: rbac.authorization.k8s.io/v1beta1
+kind: ClusterRole
+metadata:
+  name: rbac-proxy-metrics-prom
+rules:
+  - nonResourceURLs: ["/metrics"]
+    verbs: ["get"]
+---
+apiVersion: rbac.authorization.k8s.io/v1beta1
+kind: ClusterRoleBinding
+metadata:
+  name: rbac-proxy-metrics-prom-rb
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: rbac-proxy-metrics-prom
+subjects:
+  - kind: ServiceAccount
+    name: prometheus-k8s
+    namespace: openshift-monitoring
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRole
+metadata:
+  name: rbac-proxy-reviews-prom
+rules:
+  - apiGroups: ["authentication.k8s.io"]
+    resources:
+      - tokenreviews
+    verbs: ["create"]
+  - apiGroups: ["authorization.k8s.io"]
+    resources:
+      - subjectaccessreviews
+    verbs: ["create"]
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: rbac-proxy-reviews-prom-rb
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: rbac-proxy-reviews-prom
+subjects:
+  - kind: ServiceAccount
+    name: controller
+    namespace: knative-serving
+---
+apiVersion: v1
+kind: Service
+metadata:
+  labels:
+    name: activator-sm-service
+  name: activator-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: activator
+---
+apiVersion: v1
+kind: Service
+metadata:
+  labels:
+    name: autoscaler-sm-service
+  name: autoscaler-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: autoscaler
+---
+apiVersion: v1
+kind: Service
+metadata:
+  labels:
+    name: autoscaler-hpa-sm-service
+  name: autoscaler-hpa-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: autoscaler-hpa
+---
+apiVersion: v1
+kind: Service
+metadata:
+  labels:
+    name: controller-sm-service
+  name: controller-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: controller
+---
+apiVersion: v1
+kind: Service
+metadata:
+  labels:
+    name: domain-mapping-sm-service
+  name: domain-mapping-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: domain-mapping
+---
+apiVersion: v1
+kind: Service
+metadata:
+  labels:
+    name: domainmapping-webhook-sm-service
+  name: domainmapping-webhook-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: domainmapping-webhook
+---
+apiVersion: v1
+kind: Service
+metadata:
+  labels:
+    name: webhook-sm-service
+  name: webhook-sm-service
+  namespace: knative-serving
+spec:
+  ports:
+    - name: https
+      port: 8444
+  selector:
+    app: webhook
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: activator-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+    bearerTokenSecret:
+      key: ""
+    port: https
+    scheme: https
+    tlsConfig:
+        insecureSkipVerify: true
+  namespaceSelector:
+    matchNames:
+    - knative-serving
+  selector:
+    matchLabels:
+      name: activator-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: autoscaler-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        insecureSkipVerify: true
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: autoscaler-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: autoscaler-hpa-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        insecureSkipVerify: true
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: autoscaler-hpa-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: controller-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        insecureSkipVerify: true
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: controller-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: domain-mapping-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        insecureSkipVerify: true
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: domain-mapping-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: domainmapping-webhook-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        insecureSkipVerify: true
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: domainmapping-webhook-sm-service
+---
+apiVersion: monitoring.coreos.com/v1
+kind: ServiceMonitor
+metadata:
+  name: webhook-sm
+  namespace: knative-serving
+spec:
+  endpoints:
+    - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
+      bearerTokenSecret:
+        key: ""
+      port: https
+      scheme: https
+      tlsConfig:
+        insecureSkipVerify: true
+  namespaceSelector:
+    matchNames:
+      - knative-serving
+  selector:
+    matchLabels:
+      name: webhook-sm-service
